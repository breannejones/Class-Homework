{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uE2bkWvcsDga-0wciDByj5bn2w-EF9N2","timestamp":1709332757338}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"669SsCN3N6F8"},"source":["# MKTG 685 - Machine Learning in Marketing\n","# Essentials of Natural Language Processing\n","# NLP Basic II - Sentiment Analysis\n","\n","Hyunhwan \"Aiden\" Lee\n","\n","> Assistant Professor of Marketing, College of Business, California State University Long Beach\n","\n","Copyright (c) 2021 ~ present"]},{"cell_type":"markdown","source":["In this homework, we will run Naive Bayes for sentiment analysis with preprocessing that we studied in our first week class. Through this homework, you will review the first week lesson. Please follow the code in the next lines.\n","\n","To complete this assingment, please \"save a copy in Drive\" and complete the codes.\n","Also, run all the codes before you submit.\n","Then, submit your \"ipynb\" file on Canvas.\n","\n","If you have any questions, please contact to our TA."],"metadata":{"id":"tVejd05tqm4_"}},{"cell_type":"code","metadata":{"id":"Dm43U3r7T99G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709677707954,"user_tz":480,"elapsed":3777,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}},"outputId":"a1113c8c-01e1-4e75-80ea-3d496f2c9a0b"},"source":["# DO NOT CHANGE THIS CODE\n","import string\n","import re\n","\n","# NLTK\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('twitter_samples')\n","from nltk.corpus import stopwords\n","from nltk.corpus import twitter_samples\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"]}]},{"cell_type":"markdown","metadata":{"id":"-ZDopnSrvaJV"},"source":["# Naïve Bayes for Sentiment Analysis\n","\n","The following questions are about Naïve Bayes model for sentiment analysis."]},{"cell_type":"markdown","source":["Q) In this assignment, we will use Porter stemmer. Please fill the code in the cell below:"],"metadata":{"id":"kPEtcHkjBZfg"}},{"cell_type":"code","metadata":{"id":"WadslHVcwpJ_","executionInfo":{"status":"ok","timestamp":1709677711904,"user_tz":480,"elapsed":212,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}}},"source":["# Complete the following code\n","STOPWORD = stopwords.words('english')\n","# for stemming use Porter stemmer\n","STEMMER = PorterStemmer()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"zWm2o28YwRrN","executionInfo":{"status":"ok","timestamp":1709677713403,"user_tz":480,"elapsed":153,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}}},"source":["# DO NOT CHANGE THIS CODE\n","# Create a function to do preprocessing.\n","def preprocessing_text(text):\n","  # for Twitter data\n","  # remove if text begins with @\n","  text = re.sub(r'(\\s)@\\w+', '', text)\n","  # remove stock market tickers like $GE\n","  text = re.sub(r'\\$\\w*', '', text)\n","  # remove old style retext text \"RT\"\n","  text = re.sub(r'^RT[\\s]+', '', text)\n","  # remove hyperlinks\n","  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","  # remove hashtags\n","  # only removing the hash # sign from the word\n","  text = re.sub(r'#', '', text)\n","  # tokenize texts\n","  text_tokens = word_tokenize(text)\n","  texts_clean = []\n","  for word in text_tokens:\n","    if (word not in STOPWORD and  # remove stopwords\n","            word not in string.punctuation):  # remove punctuation\n","        stem_word = STEMMER.stem(word)  # stemming word\n","        texts_clean.append(stem_word)\n","  return ' '.join(texts_clean)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["The following code is your train dataset."],"metadata":{"id":"v1FhpiuRPvTW"}},{"cell_type":"code","source":["# DO NOT CHANGE THIS CODE\n","# This is your train dataset\n","train_positive = [\n","    'good car ever',\n","    'best vehicle ever',\n","    'awesome design',\n","    'sharp handling',\n","    'great transmission'\n","]\n","\n","train_negative = [\n","    'bad car ever',\n","    'poor car design',\n","    'weird design',\n","    'worst and slow transmission',\n","    'hard handling'\n","]"],"metadata":{"id":"E8J4qeMIvJSq","executionInfo":{"status":"ok","timestamp":1709677717461,"user_tz":480,"elapsed":187,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Q) Using the train data above, please run preprocessing using the predefined 'preprocessing_text' function."],"metadata":{"id":"KWvjUKEU029b"}},{"cell_type":"code","source":["# do preprocessing for the data\n","all_positive_cleaned = [preprocessing_text(text) for text in train_positive]\n","all_negative_cleaned = [preprocessing_text(text) for text in train_negative]"],"metadata":{"id":"PyaeTNuyvJZr","executionInfo":{"status":"ok","timestamp":1709677719823,"user_tz":480,"elapsed":230,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["If you do correctly, the expected out put in the following cell should be\n","\n","['good car ever', 'best vehicl ever', 'awesom design', 'sharp handl', 'great transmiss']  \n","['bad car ever', 'poor car design', 'weird design', 'worst slow transmiss', 'hard handl']"],"metadata":{"id":"hBUQBq-wCdyF"}},{"cell_type":"code","source":["# DO NOT CHANGE THIS CODE\n","print(all_positive_cleaned)\n","print(all_negative_cleaned)"],"metadata":{"id":"DLIQpachCZyY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709677721525,"user_tz":480,"elapsed":162,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}},"outputId":"951fbc23-12f1-4e13-f438-38c41b1772f4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['good car ever', 'best vehicl ever', 'awesom design', 'sharp handl', 'great transmiss']\n","['bad car ever', 'poor car design', 'weird design', 'worst slow transmiss', 'hard handl']\n"]}]},{"cell_type":"markdown","source":["Q) Combine negative and positive data for train set."],"metadata":{"id":"qUZweRO-KRdH"}},{"cell_type":"code","source":["# Complete the following code\n","train = []\n","for text in all_positive_cleaned:\n","  train.append((text, 'pos'))\n","for text in all_negative_cleaned:\n","  train.append((text, 'neg'))"],"metadata":{"id":"Wn2MxpDpC3RH","executionInfo":{"status":"ok","timestamp":1709677724168,"user_tz":480,"elapsed":150,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["If you do correctly, the expected out put in the following cell should be\n","\n","('good car ever', 'pos')\n","\n","('weird design', 'neg')"],"metadata":{"id":"5TYg4UuUKcsJ"}},{"cell_type":"code","source":["# DO NOT CHANGE THIS CODE\n","print(train[0])\n","print(train[7])"],"metadata":{"id":"wbcdFC-LC3bP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709677728599,"user_tz":480,"elapsed":179,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}},"outputId":"0bc8e164-0540-4284-80b7-2e720206db24"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["('good car ever', 'pos')\n","('weird design', 'neg')\n"]}]},{"cell_type":"markdown","metadata":{"id":"RIIZoEelDnbj"},"source":["Let's train Naive Bayes model."]},{"cell_type":"code","metadata":{"id":"e8grj3vhDnbj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709677909581,"user_tz":480,"elapsed":138,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}},"outputId":"f5262b64-158c-4f9a-8fd3-c6b695186fab"},"source":["# DO NOT CHANGE THIS CODE\n","dtmvector = CountVectorizer()\n","train_x_dtm = dtmvector.fit_transform(all_positive_cleaned+all_negative_cleaned)\n","tfidf_transformer = TfidfTransformer()\n","train_x_tfidf = tfidf_transformer.fit_transform(train_x_dtm)\n","print(train_x_tfidf.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 17)\n"]}]},{"cell_type":"markdown","source":["Q) Create labels (i.e., positive: 1, negative : 0)."],"metadata":{"id":"Lwj03NG_KvK9"}},{"cell_type":"code","metadata":{"id":"dLKRN7sFEGRa","executionInfo":{"status":"ok","timestamp":1709678104090,"user_tz":480,"elapsed":205,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}}},"source":["# Complete the following code\n","train_y = np.append(np.ones(len(all_positive_cleaned)), np.zeros(len(all_negative_cleaned)))\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"QI2e_iOEEGRa","colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"status":"ok","timestamp":1709678111605,"user_tz":480,"elapsed":157,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}},"outputId":"0d1b6207-66d7-44fa-bc2f-af433fc9853a"},"source":["# DO NOT CHANGE THIS CODE\n","mod = MultinomialNB()\n","mod.fit(train_x_tfidf, train_y)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["Let's test using new texts."],"metadata":{"id":"NOGBtgV5EhFS"}},{"cell_type":"code","source":["# DO NOT CHANGE THIS CODE\n","def run_naive_bayes_sentiment(text):\n","  text_dtm = dtmvector.transform([text])\n","  text_tfidf = tfidf_transformer.transform(text_dtm)\n","  if mod.predict(text_tfidf)[0] == 0:\n","    print('The sentence [%s] is negative!' % (text))\n","  else:\n","    print('The sentence [%s] is positive!' % (text))"],"metadata":{"id":"Z756eF-iEhPr","executionInfo":{"status":"ok","timestamp":1709678129801,"user_tz":480,"elapsed":164,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["First, let's use \"bad car ever.\""],"metadata":{"id":"Ai05_Q10Gb13"}},{"cell_type":"code","source":["# DO NOT CHANGE THIS CODE\n","run_naive_bayes_sentiment('bad car ever')"],"metadata":{"id":"mo6hVT6sEhUm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709678132862,"user_tz":480,"elapsed":163,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}},"outputId":"fb08b349-10dd-4d42-c481-a7f21bd04d5c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["The sentence [bad car ever] is negative!\n"]}]},{"cell_type":"markdown","source":["Then, let's test \"ugly vehicle ever\" and \"nice car.\""],"metadata":{"id":"vJRvMEN1GhX9"}},{"cell_type":"code","source":["# DO NOT CHANGE THIS CODE\n","run_naive_bayes_sentiment('ugly vehicle ever')"],"metadata":{"id":"nGYy8aKbE8i1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709678137186,"user_tz":480,"elapsed":173,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}},"outputId":"33cf8505-d4a1-441f-ae77-686b6209d32d"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["The sentence [ugly vehicle ever] is positive!\n"]}]},{"cell_type":"code","source":["# DO NOT CHANGE THIS CODE\n","run_naive_bayes_sentiment('nice car')"],"metadata":{"id":"xTdzvtBoHMlU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709678141364,"user_tz":480,"elapsed":265,"user":{"displayName":"Breanne Jones","userId":"14720758547105754218"}},"outputId":"e1918f5e-2850-4580-8c29-5a94cdddd89b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["The sentence [nice car] is negative!\n"]}]},{"cell_type":"markdown","source":["Q) Why did naive Bayes classifer evalaute 'ugly vehicle ever' as positive? And why is \"nice car\" negative?"],"metadata":{"id":"K2qIscmsE1_l"}},{"cell_type":"markdown","source":["Your Answer: The code associates the word ever with positive because it appears more in the positive training data than the negative. It also associates the word car with negative because it appears more in the negative training data than the positive. Since Naive Bayes is judging the probabilites of each word in training set, each word has the same weight.\n","\n","\n"],"metadata":{"id":"-n-1n15sE2BZ"}},{"cell_type":"markdown","source":["Q) What can be a solution to avoid the issue above?"],"metadata":{"id":"t-Kg6hajIDZM"}},{"cell_type":"markdown","source":["Your Answer: In the training data we can remove the word car and ever completely. The negative and positive connotated words, such as \"nice\", \"ugly\", \"worst\", etc. are the ones that should be included in the training data. We can also give the model much larger data sets to prevent assigning words wit neutral connotations positive and negative ones."],"metadata":{"id":"wWiwPfykIDbc"}}]}