{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiiEU2DoXylD"
      },
      "source": [
        "# MKTG 685 - Machine Learning in Marketing\n",
        "# NLP Basic I - Preprocessing - Assignment\n",
        "\n",
        "Dr. Hyunhwan \"Aiden\" Lee\n",
        "\n",
        "> Assistant Professor of Marketing, California State University Long Beach   \n",
        "\n",
        "Copyright 2021 ~ present.\n",
        "\n",
        "\n",
        "DO NOT share this code without the author's permission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDhFfC-hYPFA"
      },
      "source": [
        "## Assignment\n",
        "\n",
        "Mike is a marketing manager of a cruise company. His company uses Net Promoter Score  to measure customer satisfaction. The customers also answer open-end questions. The following is one of the open-end survey answers. Please answer the following questions.\n",
        "\n",
        "\n",
        "> Our waiter at the main dining room was fantastic and so was his helper. I just wish the menu had better selections on it... It would have been great to have a couple of steak-selections to pick from that you didn't have to pay more money. And the size of the portions were really small!! My husband ordered the tiger shrimp and there were 4 pieces of shrimp on the plate. My husband is a big man and our waiter saw the look on my husband's face and brought him another shrimp dinner. That was wonderful!!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alUjtSUL9fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5cf60f-4a0e-48e3-9d8a-f12e8815a8a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "##### DO NOT CHANGE CODES IN THIS CELL #####\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text_review = \"Our waiter at the main dining room was fantastic and so was his helper. I just wish the menu had better selections on it... It would have been great to have a couple of steak-selections to pick from that you didn't have to pay more money. And the size of the portions were really small!! My husband ordered the tiger shrimp and there were 4 pieces of shrimp on the plate. My husband is a big man and our waiter saw the look on my husband's face and brought him another shrimp dinner. That was wonderful!!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJaJ2UPgYcqS"
      },
      "source": [
        "1.\tTo analyze the sentence, Mike used `sent_tokenize` from NLTK package. What is the number of sentences of the customer feedback? (Store the result in `list_review`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCcsZMGcYeGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8bbcd3-840a-4ee3-859c-8cebdbf51baa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Our waiter at the main dining room was fantastic and so was his helper.',\n",
              " 'I just wish the menu had better selections on it...',\n",
              " \"It would have been great to have a couple of steak-selections to pick from that you didn't have to pay more money.\",\n",
              " 'And the size of the portions were really small!!',\n",
              " 'My husband ordered the tiger shrimp and there were 4 pieces of shrimp on the plate.',\n",
              " \"My husband is a big man and our waiter saw the look on my husband's face and brought him another shrimp dinner.\",\n",
              " 'That was wonderful!',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "list_review = sent_tokenize(text_review)\n",
        "list_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmbK7fh_Yf2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3880eac4-5b66-4b59-d426-998a1ce52673"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "##### DO NOT CHANGE CODES IN THIS CELL #####\n",
        "len(list_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAnBH9dYYkP9"
      },
      "source": [
        "2.\tWhat is the correct number of sentences? Is the number of sentences from `sent_tokenize` the same as the correct number of sentences?  If not, why this happens? Also, to correct the issue, what can we do? Write your Python code to correct it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct number of sentences is 7. We are given 8 sentences because the code counts a sentence as having one item of sentence ending punctuation, therefore the extra '!' at the end of the 7th sentence counts as it's own setence."
      ],
      "metadata": {
        "id": "xkjJ74ggG18Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "for sent in list_review:\n",
        "  if len(sent)<2:\n",
        "    list_review.remove(sent)"
      ],
      "metadata": {
        "id": "nJJGCB3AJ0gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOBWgS69YrfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6dc1058-cd4b-4428-831a-d912be6df695"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Our waiter at the main dining room was fantastic and so was his helper.',\n",
              " 'I just wish the menu had better selections on it...',\n",
              " \"It would have been great to have a couple of steak-selections to pick from that you didn't have to pay more money.\",\n",
              " 'And the size of the portions were really small!!',\n",
              " 'My husband ordered the tiger shrimp and there were 4 pieces of shrimp on the plate.',\n",
              " \"My husband is a big man and our waiter saw the look on my husband's face and brought him another shrimp dinner.\",\n",
              " 'That was wonderful!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "##### DO NOT CHANGE CODES IN THIS CELL #####\n",
        "list_review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgmgIiLtYpRp"
      },
      "source": [
        "3.\tIn the customer feedback, which is the longest sentence?  How many words are in the longest sentence? And how can you find it? (Use `word_tokenize` in NLTK package and store as below and print them.)\n",
        "\n",
        "\n",
        "* longest sentence: `longest_sentence`\n",
        "* length of the longest sentence: `length`\n",
        "* total number of words of the longest sentence: `length_words`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvQCkyRC2YSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d82e73-d6d0-466f-ae12-61252904ef8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'would', 'have', 'been', 'great', 'to', 'have', 'a', 'couple', 'of', 'steak-selections', 'to', 'pick', 'from', 'that', 'you', 'did', \"n't\", 'have', 'to', 'pay', 'more', 'money', '.']\n"
          ]
        }
      ],
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "from nltk.tokenize import word_tokenize\n",
        "sent3_words = word_tokenize(list_review[2])\n",
        "print(sent3_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "longest_sentence = ''\n",
        "length = 0"
      ],
      "metadata": {
        "id": "K2SkYbj2w1eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1_words = word_tokenize(list_review[0])\n",
        "sent2_words = word_tokenize(list_review[1])\n",
        "sent4_words = word_tokenize(list_review[3])\n",
        "sent5_words = word_tokenize(list_review[4])\n",
        "sent6_words = word_tokenize(list_review[5])\n",
        "sent7_words = word_tokenize(list_review[6])"
      ],
      "metadata": {
        "id": "gD3Y3NRPIDxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sent1_words), len(sent2_words),len(sent3_words),len(sent4_words),len(sent5_words),len(sent6_words),len(sent7_words),)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoU4j4fAIU55",
        "outputId": "fc140a85-f25e-4fbb-92b2-3432f24984db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 11 24 11 17 24 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "longest_sentence = list_review[2]\n",
        "length = len(list_review[2])\n",
        "length_words = len(sent3_words)"
      ],
      "metadata": {
        "id": "iC-mNaTeIh18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvZnqR-AYjj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c661d5-fb34-49fd-dfd0-1b76f1f1cdbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It would have been great to have a couple of steak-selections to pick from that you didn't have to pay more money. 114 24\n"
          ]
        }
      ],
      "source": [
        "##### DO NOT CHANGE CODES IN THIS CELL #####\n",
        "print(longest_sentence, length, length_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsvaY6xJA8j0"
      },
      "source": [
        "4.\tMike wants to use `WordPunctTokenizer` from NLTK. Then, how many words are in the longest sentence? Print the number of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkZxb2VhZhbg"
      },
      "outputs": [],
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "sent3_words_wpt = WordPunctTokenizer().tokenize(list_review[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sent3_words_wpt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSNfy-PyJZgm",
        "outputId": "829b74f0-5892-450b-eb34-c0d274eee631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EyOT9d6A_4U"
      },
      "source": [
        "5.\tIf Mike uses `word_tokenize`, what is the number of words in the longest sentence? Print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK8Cq0JsZe0i"
      },
      "outputs": [],
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "from nltk.tokenize import word_tokenize\n",
        "sent3_words_wt = word_tokenize(list_review[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sent3_words_wt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzLO8bRKJ1d1",
        "outputId": "aaaf6058-ecaa-4004-9d6d-9b02b0618d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEMBa9BhBvdW"
      },
      "source": [
        "6.\tIf Mike uses `TreebankWordTokenizer`, what is the number of words in the longest sentence? Print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3ObQLmLZn_L"
      },
      "outputs": [],
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "sent3_words_TWT = TreebankWordTokenizer().tokenize(list_review[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sent3_words_TWT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rEtYjJSKVUU",
        "outputId": "9af462ba-9c8b-4ac0-947e-64bc8118bdb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhXfmKhety_g"
      },
      "source": [
        "7.\tWhat are the differences between these three tokenizers? Show the results from each tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAyxCDr2ZgV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99c7b1e-8ad9-4f83-d5fc-60c507fcd722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "24\n",
            "24\n"
          ]
        }
      ],
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "print(len(sent3_words_wpt))\n",
        "print(len(sent3_words_wt))\n",
        "print(len(sent3_words_TWT))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordPunctTokenizer gives us 27 total words in sentence 3 while WordTokenizer and TreebankWordTokenizer give us 24 total words in sentence 3."
      ],
      "metadata": {
        "id": "DFfBa4Y-KiuW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhs0F99Ot4gK"
      },
      "source": [
        "8.\tHow many nouns (exclude pronouns) are in the longest sentence? (use `word_tokenize` from NLTK for word tokenizer and store the nouns to `list_nouns`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVsQ2ZfUZwxP"
      },
      "outputs": [],
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "from nltk import pos_tag\n",
        "tagged_words = nltk.pos_tag(word_tokenize(list_review[2]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9ShIQBpDNL3",
        "outputId": "2a2e470d-8eae-424f-8073-1557c03e1861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('It', 'PRP'), ('would', 'MD'), ('have', 'VB'), ('been', 'VBN'), ('great', 'JJ'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('couple', 'NN'), ('of', 'IN'), ('steak-selections', 'NNS'), ('to', 'TO'), ('pick', 'VB'), ('from', 'IN'), ('that', 'IN'), ('you', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('have', 'VB'), ('to', 'TO'), ('pay', 'VB'), ('more', 'JJR'), ('money', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_nouns = []\n",
        "for word in tagged_words:\n",
        "  if word[1] == 'NN':\n",
        "    list_nouns.append(word)\n",
        "  if word[1] == 'NNS':\n",
        "    list_nouns.append(word)"
      ],
      "metadata": {
        "id": "FPJ-qCoPCc81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### DO NOT CHANGE CODES IN THIS CELL #####\n",
        "len(list_nouns)"
      ],
      "metadata": {
        "id": "BvX0EiulMG82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfd8fd4-bedb-47d5-a698-de3d219b8722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTaba9u40BKR"
      },
      "source": [
        "9.\tHow many nouns, verbs, adverbs, and adjectives are in the customer feedback? (Use `TreebankWordTokenizer` from NLTK for word tokenizer and find the number of each target POS tag.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUy2osuqX00F"
      },
      "outputs": [],
      "source": [
        "##### DO NOT CHANGE CODES IN THIS CELL #####\n",
        "list_nouns = []\n",
        "list_verbs = []\n",
        "list_adverbs = []\n",
        "list_adjectives = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_review_twt = TreebankWordTokenizer().tokenize(text_review)"
      ],
      "metadata": {
        "id": "oWWOcXXBDzmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_words_twt = pos_tag(list_review_twt)\n",
        "print(tagged_words_twt)\n",
        "print(len(tagged_words_twt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhj7S9lOFiSx",
        "outputId": "a033b17d-8e75-43b0-d483-9b8c5b11dce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Our', 'PRP$'), ('waiter', 'NN'), ('at', 'IN'), ('the', 'DT'), ('main', 'JJ'), ('dining', 'NN'), ('room', 'NN'), ('was', 'VBD'), ('fantastic', 'JJ'), ('and', 'CC'), ('so', 'RB'), ('was', 'VBD'), ('his', 'PRP$'), ('helper.', 'NN'), ('I', 'PRP'), ('just', 'RB'), ('wish', 'VB'), ('the', 'DT'), ('menu', 'NN'), ('had', 'VBD'), ('better', 'JJR'), ('selections', 'NNS'), ('on', 'IN'), ('it', 'PRP'), ('...', ':'), ('It', 'PRP'), ('would', 'MD'), ('have', 'VB'), ('been', 'VBN'), ('great', 'JJ'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('couple', 'NN'), ('of', 'IN'), ('steak-selections', 'NNS'), ('to', 'TO'), ('pick', 'VB'), ('from', 'IN'), ('that', 'IN'), ('you', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('have', 'VB'), ('to', 'TO'), ('pay', 'VB'), ('more', 'JJR'), ('money.', 'NN'), ('And', 'CC'), ('the', 'DT'), ('size', 'NN'), ('of', 'IN'), ('the', 'DT'), ('portions', 'NNS'), ('were', 'VBD'), ('really', 'RB'), ('small', 'JJ'), ('!', '.'), ('!', '.'), ('My', 'PRP$'), ('husband', 'NN'), ('ordered', 'VBD'), ('the', 'DT'), ('tiger', 'NN'), ('shrimp', 'NN'), ('and', 'CC'), ('there', 'EX'), ('were', 'VBD'), ('4', 'CD'), ('pieces', 'NNS'), ('of', 'IN'), ('shrimp', 'NN'), ('on', 'IN'), ('the', 'DT'), ('plate.', 'NN'), ('My', 'NNP'), ('husband', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('big', 'JJ'), ('man', 'NN'), ('and', 'CC'), ('our', 'PRP$'), ('waiter', 'NN'), ('saw', 'VBD'), ('the', 'DT'), ('look', 'NN'), ('on', 'IN'), ('my', 'PRP$'), ('husband', 'NN'), (\"'s\", 'POS'), ('face', 'NN'), ('and', 'CC'), ('brought', 'VBD'), ('him', 'PRP'), ('another', 'DT'), ('shrimp', 'NN'), ('dinner.', 'NN'), ('That', 'WDT'), ('was', 'VBD'), ('wonderful', 'JJ'), ('!', '.'), ('!', '.')]\n",
            "103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from types import WrapperDescriptorType\n",
        "##### COMPLETE YOUR CODES #####\n",
        "for word, pos in tagged_words_twt:\n",
        "    if pos.startswith('N'):\n",
        "        list_nouns.append(word)\n",
        "    elif pos.startswith('V'):\n",
        "        list_verbs.append(word)\n",
        "    elif pos.startswith('RB'):\n",
        "        list_adverbs.append(word)\n",
        "    elif pos.startswith('JJ'):\n",
        "        list_adjectives.append(word)"
      ],
      "metadata": {
        "id": "YccSzvwwMbI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### DO NOT CHANGE CODES IN THIS CELL #####\n",
        "print(len(list_nouns))\n",
        "print(len(list_verbs))\n",
        "print(len(list_adverbs))\n",
        "print(len(list_adjectives))"
      ],
      "metadata": {
        "id": "4rRFwRk2MWL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dea7a70-a7f7-426d-f7cf-1f4db25a061c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n",
            "25\n",
            "5\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PRrM9UXaGxc"
      },
      "source": [
        "10.\tUse regular expressions (RE in Python) from the previous question to get the number of nouns, verbs, adverbs, and adjectives each. (Hint. Use re.match and also following URL https://docs.python.org/3/library/re.html, https://docs.python.org/3/library/re.html#functions, https://docs.python.org/3/library/re.html#search-vs-match )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0WM4Fo3647h"
      },
      "outputs": [],
      "source": [
        "##### DO NOT CHANGE CODES IN THIS CELL #####\n",
        "list_nouns = []\n",
        "list_verbs = []\n",
        "list_adverbs = []\n",
        "list_adjectives = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "import re\n",
        "\n",
        "noun_pattern = re.compile(r'^N')\n",
        "verb_pattern = re.compile(r'^V')\n",
        "adv_pattern = re.compile(r'^RB')\n",
        "adj_pattern = re.compile(r'^JJ')"
      ],
      "metadata": {
        "id": "AE6XMOyTMlyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, pos in tagged_words_twt:\n",
        "  if noun_pattern.match(pos):\n",
        "    list_nouns.append(word)\n",
        "  elif verb_pattern.match(pos):\n",
        "    list_verbs.append(word)\n",
        "  elif adv_pattern.match(pos):\n",
        "    list_adverbs.append(word)\n",
        "  elif adj_pattern.match(pos):\n",
        "    list_adjectives.append(word)"
      ],
      "metadata": {
        "id": "3weotkvJZ3pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### DO NOT CHANGE CODES IN THIS CELL #####\n",
        "print(len(list_nouns))\n",
        "print(len(list_verbs))\n",
        "print(len(list_adverbs))\n",
        "print(len(list_adjectives))"
      ],
      "metadata": {
        "id": "JXju-VKuMl0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a389729b-9d8f-4042-9159-00809d123b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n",
            "25\n",
            "5\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwQuB6-CqsnD"
      },
      "source": [
        "11.\t If Mike wants to use regular expressions for word tokenization by **space**, what is the number of words in the longest sentence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSthf6xFaI2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11deb9f-5da3-4f86-cd61-dd6a76364bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Our waiter at the main dining room was fantastic and so was his helper', ' I just wish the menu had better selections on it', '', '', \" It would have been great to have a couple of steak-selections to pick from that you didn't have to pay more money\", ' And the size of the portions were really small', '', ' My husband ordered the tiger shrimp and there were 4 pieces of shrimp on the plate', \" My husband is a big man and our waiter saw the look on my husband's face and brought him another shrimp dinner\", ' That was wonderful', '', '']\n"
          ]
        }
      ],
      "source": [
        "##### COMPLETE YOUR CODES #####\n",
        "sentences = re.split('[.!]', text_review)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sentences = []\n",
        "for sent in sentences:\n",
        "  if len(sent)>1:\n",
        "    clean_sentences.append(sent)\n",
        "print(clean_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKZpuE00_OLx",
        "outputId": "82a2b1c7-78e5-4d21-818f-ff2a4c6cccc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Our waiter at the main dining room was fantastic and so was his helper', ' I just wish the menu had better selections on it', \" It would have been great to have a couple of steak-selections to pick from that you didn't have to pay more money\", ' And the size of the portions were really small', ' My husband ordered the tiger shrimp and there were 4 pieces of shrimp on the plate', \" My husband is a big man and our waiter saw the look on my husband's face and brought him another shrimp dinner\", ' That was wonderful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
        "\n",
        "sent1_words = tokenizer.tokenize(clean_sentences[0])\n",
        "sent2_words = tokenizer.tokenize(clean_sentences[1])\n",
        "sent3_words = tokenizer.tokenize(clean_sentences[2])\n",
        "sent4_words = tokenizer.tokenize(clean_sentences[3])\n",
        "sent5_words = tokenizer.tokenize(clean_sentences[4])\n",
        "sent6_words = tokenizer.tokenize(clean_sentences[5])\n",
        "sent7_words = tokenizer.tokenize(clean_sentences[6])"
      ],
      "metadata": {
        "id": "H4AUuhD9_q5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sent1_words), len(sent2_words), len(sent3_words), len(sent4_words), len(sent5_words), len(sent6_words), len(sent7_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hRlLHaJB3dU",
        "outputId": "47fdefaa-db1e-4980-d7e1-c74e2f3bb4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14 10 24 9 16 23 3\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}